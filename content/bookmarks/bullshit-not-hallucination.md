---
title: Bullshit, not hallucination
date: "2024-06-20T07:20:01Z"
emoji: "\U0001F4A9"
publishDate: "2024-06-08T00:00:00Z"
bookmarkOf: https://link.springer.com/article/10.1007/s10676-024-09775-5
references:
  bookmark:
    url: https://link.springer.com/article/10.1007/s10676-024-09775-5
    type: entry
    name: ChatGPT is Bullshit (from Ethics and Information Technology)
    summary: 'Ethics and Information Technology - Recently, there has been considerable
      interest in large language models: machine learning systems which produce human-like
      text and dialogue. Applications of...'
    author: Slater, Joe
tags:
- AI
- Philosophy
---
A well reasoned paper on why AI-generated falsehoods should be called bullshit, not hallucinations.

The rise of generative AI must be an absolute joy for philosophers in that space; what a wealth of new concepts and ways to consider what it means to be human! The authors of this paper seem to have had some fun _and_ provided a new and excellent viewpoint too.

In short: if bullshit is delivering information without concern for its truth then ChatGPT (and its ilk) are bullshitters. All those who use LLM output without working to correct any & all inaccuracies are, by the transitive property of â€œ(not) being concerned about the truthâ€, also bullshitters.

### Highlights

> at minimum, the outputs of LLMs like ChatGPT are soft bullshit:

I am fully convinced by the arguments for this put forward in this paper; so Iâ€™ll be referring to erroneous LLM output as ~hallucination~ bullshit from now on.

---

> ChatGPT is a bullshit machine

---

> The very same process occurs when its outputs happen to be true.

An excellent point; if an inaccuracy in an LLMs output is called a hallucination then why is an accuracy anything different?

---

> these are often called â€œAI hallucinationsâ€. We argue that these falsehoods, and the overall activity of large language models, is better understood as _bullshit_ in the sense explored by Frankfurt (On Bullshit, Princeton, 2005):

---

> because they are designed to produce text that _looks_ truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.

---

> Descriptions of new technology, including metaphorical ones, guide policymakersâ€™ and the publicâ€™s understanding of new technology;

This is much more accurate and relevant than, I think, many people understand; Iâ€™m _certain_ itâ€™s why the word â€œhallucinationâ€ was originally coined (by AI companies, Iâ€™d wager) to describe the easily detected fallacies LLMs produce.

In pre-LLM widespread use of the word a person is _affected by_ (desirably or otherwise) hallucinations, it is external action, it can sometimes be â€œnot your faultâ€. Whereas â€œbullshitâ€ is intrinsic â€” _you_ chose to bullshit when asked a simple question.

Neither of these words really describes whatâ€™s going on inside an LLM when a falsehood arises, but the associations from â€œhallucinationâ€ are much more favourable to the profitability of an LLM company than â€œbullshitâ€ could ever be.

---

> We draw a distinction between two sorts of bullshit, which we call â€˜hardâ€™ and â€˜softâ€™ bullshit, where the former requires an active attempt to deceive the reader or listener as to the nature of the enterprise, and the latter only requires a lack of concern for truth.

---

> we call bullshit on ChatGPT.

---

> ChatGPT may indeed produce hard bullshit: if we view it as having intentions (for example, in virtue of how it is designed), then the fact that it is designed to give the impression of concern for truth qualifies it as attempting to mislead the audience about its aims, goals, or agenda.

---

> The problem here isnâ€™t that large language models hallucinate, lie, or misrepresent the world in some way. Itâ€™s that they are not designed to represent the world at all; instead, they are designed to convey convincing lines of text.

---

> I entered a pun competition and because I really wanted to win, I submitted ten entries. I was sure one of them would win, but no pun in ten did.

I love that the authors worked this superb Dad joke into this paper ðŸ˜‚

---

> Frankfurt understands bullshit to be characterized not by an intent to deceive but instead by a reckless disregard for the truth.

---

> ### Bullshit (general)
> 
> Any utterance produced where a speaker has indifference towards the truth of the utterance.
> 
> ### Hard bullshit
> 
> Bullshit produced with the intention to mislead the audience about the uttererâ€™s agenda.
> 
> ### Soft bullshit
> 
> Bullshit produced without the intention to mislead the hearer regarding the uttererâ€™s agenda.

---

> whether or not ChatGPT has agency, its creators and users do. And what they produce with it, we will argue, is bullshit.

---

> if something is bullshit to start with, then its repetition â€œis bullshit as he \[or it\] repeats it, insofar as it was originated by someone who was unconcerned with whether what he was saying is true or falseâ€

---

> Calling these inaccuracies â€˜bullshitâ€™ rather than â€˜hallucinationsâ€™ isnâ€™t just more accurate (as weâ€™ve argued); itâ€™s good science and technology communication in an area that sorely needs it.
